{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Search - All Credit to Khan Academy\n",
    "\n",
    "**Problem:** An array with n-elements is given. You need to find the index of element (target) that you need to search in the array. The array and the target can be provided as inputs to the Binary Search method, while the index of the target element should be returned as output. It should return -1, if no such element is found in the input array.\n",
    "\n",
    "**Psuedo code**\n",
    "\n",
    "\n",
    "\n",
    "> 1. Let min = 0 and max = n - 1\n",
    "2. If min > max, then the target element is not present in array. Return -1.\n",
    "3. Compute guess as the average of max and min, rounded down to an integer.\n",
    "4. If array[guess] equals target, then stop. You found it! Return guess.\n",
    "5. If the guess was too low, that is array[guess] < target, set min = guess + 1.\n",
    "6. Otherwise, the guess was too high. Set max = guess - 1.\n",
    "7. Go back to step 2.\n",
    "\n",
    "**Running Time:** The running time of binary search for an array with n elements can be given by log<sub>2</sub>n + 1. \n",
    "\n",
    "**E.g.:**\n",
    "\n",
    "For 32 elements, log<sub>2</sub>32 = 5, so it will take at most 6 iterations.\n",
    "\n",
    "For 1000 elements, log<sub>2</sub>512 = 9, so it will take at most 10 iterations.\n",
    "\n",
    "**P.S.** What if n isn't a power of 2? In that case, we can look at the closest lower power of 2. For an array whose length is 1000, the closest lower power of 2 is 512, which equals 2<sup>9</sup>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(input_array, target):\n",
    "    min_val = 0\n",
    "    max_val = len(input_array) - 1\n",
    "    while True:\n",
    "        if min_val > max_val:\n",
    "            return -1\n",
    "        else:\n",
    "            guess = int((min_val + max_val)/2)\n",
    "            if input_array[guess]==target:\n",
    "                return guess\n",
    "            else:\n",
    "                if input_array[guess]<target:\n",
    "                    min_val = guess + 1\n",
    "                else:\n",
    "                    max_val = guess - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_array = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n",
    "target = 67\n",
    "\n",
    "binary_search(input_array, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Efficiency of Binary Search\n",
    "\n",
    "It is observed that whenever the number of elements double, the number of iterations increase by 1 in case of Binary Search. Please refer to following table to understand this:\n",
    "\n",
    "**n**|-|0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16\n",
    "---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "**Iteration**|-|0|1|2|2|3|3|3|3|4|4|4|4|4|4|4|4|5\n",
    "\n",
    "From the above table, it can be observed that n+1 iterations are needed for 2<sup>n</sup> inputs.\n",
    "\n",
    "For e.g., 3 + 1 iterations are needed for 8 inputs. Let's ignore 1 for now. Then we get 3 iterations for 8 inputs.\n",
    "\n",
    "As we know:\n",
    "\n",
    "2<sup>3</sup> = 8\n",
    "\n",
    "log<sub>2</sub>(2<sup>3</sup>) = log<sub>2</sub>(8)\n",
    "\n",
    "3 = log<sub>2</sub>(8). This means 3 iterations can be written as log<sub>2</sub>(8). Also, 3 + 1 iterations are needed for 8 elements. So we can say that log<sub>2</sub>(8) + 1 iterations would be needed for 8 elements. And we can also replace 8 by n, as it would follow for any number in approximation.\n",
    "\n",
    "Thus the efficiency of binary search becomes log<sub>2</sub>(n) + 1. And then if we ignore 1, we can say it's efficiency would be log<sub>2</sub>(n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptotic Notations\n",
    "\n",
    "**Running time of an algorithm - Two Ideas**\n",
    "\n",
    "1. Think of running time as function of the size of its input. Since clearly, the running time increases with the size of input.\n",
    "2. We focus on how fast a function grows with the input size. This is called **rate of growth** of the algorithm's running time. We drop the less significant terms and coefficients from the running time equation, this gives us it's rate of growth.\n",
    "\n",
    "When we drop the constant coefficients and the less significant terms, we use asymptotic notation. We'll see three forms of it: big-Θ (Theta) notation, big-O notation, and big-Ω(Omega) notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big-Θ Notation - Tight Bound\n",
    "\n",
    "When we say that a particular running time is ***Θ(n)***, we are saying that once **n** gets large enough, the running time is at least *k<sub>1</sub> ⋅ n* and at most *k<sub>2</sub> ⋅ n* for some constants *k<sub>1</sub>* and *k<sub>2</sub>*. \n",
    "\n",
    "![Big-Theta on n](./img/Big-Theta1.png)\n",
    "\n",
    "For small values of n, we don't care how the running time compares with *k<sub>1</sub> ⋅ n* or *k<sub>2</sub> ⋅ n*, but once the n gets large enough or say it is on the right side of the dashed line, the running time must be sandwiched between *k<sub>1</sub> ⋅ n* and *k<sub>2</sub> ⋅ n*.\n",
    "\n",
    "As long as these constants *k<sub>1</sub>* and *k<sub>2</sub>* exists, we say that the running time is ***Θ(n)***. The running time has upper bound as well as the lower bound.\n",
    "\n",
    "We are not restricted to just *n* in Big-Θ notation, we could have any function such as n<sup>2</sup>, nlog<sub>2</sub>n, or any other function of n. Here is how to think of a running time that is *Θ(f(n))* of some function *f(n)*.\n",
    "\n",
    "![Big-Theta on f(n)](./img/Big-Theta2.png)\n",
    "\n",
    "Once *n* gets large enough, the running time is between *k<sub>1</sub> ⋅ f(n)* and *k<sub>2</sub> ⋅ f(n)*\n",
    "\n",
    "When using Big-Θ notation, we are saying that we have an asymptotically tight bound on the running time. Asymptotic because it matters only for large values of *n* or *f(n)* and tight bound because we nailed  the running time to within a constant factor above and below.\n",
    "\n",
    "**Formal Definition:**\n",
    "\n",
    "For f:N->R+, f = Θ(g(n)) if f = O(g) and f = Ω(g).\n",
    "****\n",
    "Some common Functions growing from lowest to highest:\n",
    "\n",
    "1. Θ(1) - Constant\n",
    "2. Θ(log<sub>2</sub>n) - Logarithmic\n",
    "3. Θ(n) - Linear\n",
    "4. Θ(nlog<sub>2</sub>n) - Linarithmic\n",
    "5. Θ(n<sup>2</sup>) - Polynomial\n",
    "6. Θ(n<sup>2</sup>log<sub>2</sub>n) - Polynomial Logarithmic\n",
    "7. Θ(n<sup>3</sup>) - Higher power of Polynomial\n",
    "8. Θ(2<sup>n</sup>) - Exponential\n",
    "9. Θ(n!) - Factorial\n",
    "\n",
    "****\n",
    "\n",
    "1. Constant functions\n",
    "2. Logarithmic functions (Higher the log base, lower the growth rate)\n",
    "3. Linear functions\n",
    "4. Linearithmic functions\n",
    "5. Polynomial functions\n",
    "6. Exponential functions\n",
    "\n",
    "![f(n) = Θ(g(n)) - 1](./img/FnGn1.png)\n",
    "![f(n) = Θ(g(n)) - 2](./img/FnGn2.png)\n",
    "![f(n) = Θ(g(n)) - 3](./img/FnGn3.png)\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big-O Notation - Upper Bound\n",
    "\n",
    "When we want to say that the running time could grow at the most to some value, but could grow more slowly, we use Big-O Notation. We use Big-O Notations for **asymptotic upper bounds**, since it bounds the growth of the running time from above, for large enough input sizes.\n",
    "\n",
    "If a running time is *O(f(n))*, then for large enough *n*, the running time is at most *k ⋅ f(n)* for some constant *k*.  Here's how to think of a running time that is *O(f(n))*:\n",
    "\n",
    "![Big-O on fn](./img/BigO.png)\n",
    "\n",
    "If we say that a running time is *Θ(f(n))* in a particular situation, then its also *O(f(n))*. The converse is not necessarily true.\n",
    "\n",
    "We can say that because the worst-case running time of binary search is Θ(log<sub>2</sub>n), it's also O(log<sub>2</sub>n).\n",
    "\n",
    "**Formal Definition:**\n",
    "\n",
    "Let f (n) and g(n) be functions N+ -> R+. f = O(g) if there is a constant c > 0 and n<sub>0</sub> belongs to N+ such that\n",
    "f (n) <= c * g(n) for all n >= n<sub>0</sub>. This means that f grows no faster than g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big-Ω Notation - Lower Bound\n",
    "\n",
    "Sometimes, we want to say that an algorithm takes at least a certain amount of time, without providing an upper bound. We use big-Ω notation in such situation.\n",
    "\n",
    "If a running time is *Ω(f(n))*, then for large enough *n*, the running time is at least *k ⋅ f(n)* for some constant k. Here's how to think of a running time that is Ω(f(n)):\n",
    "\n",
    "![Big-Ω](./img/BigOmega.png)\n",
    "\n",
    "We use big-Ω notation for asymptotic lower bounds, since it bounds the growth of the running time from below for large enough input sizes.\n",
    "\n",
    "Just as Θ(f(n)) automatically implies O(f(n)), it also automatically implies Ω(f(n)). We can say that the worst-case running time of binary search is Ω(log<sub>2</sub>n).\n",
    "\n",
    "**Formal Definition:**\n",
    "\n",
    "For f:N->R+, f = Ω(g) if there is a constant c > 0 and n<sub>0</sub> belongs to N such that f (n) >= c * g(n) for all n >= n<sub>0</sub>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following expressions, indicate whether f = O(g) or f = Ω(g) or f = Θ(g):\n",
    "\n",
    "No.| f(n) | g(n) | Notation\n",
    ":---:|:---:|:---:|:---:\n",
    "1.|f (n) = n - 10| g(n) = n - 20|f = Θ(g)\n",
    "2.|f (n) = n<sup>1/2</sup>| g (n) = n<sup>2/3</sup>|f = O(g)\n",
    "3.|f (n) = n2<sup>n</sup>| g(n) = 3<sup>n</sup>|f = Ω(g)\n",
    "4.|f (n) = 2<sup>n</sup>| g(n) = 2<sup>n+1</sup>|f = Θ(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Asymp1](./img/Asymp1.png)\n",
    "![Asymp2](./img/Asymp2.png)\n",
    "![Asymp3](./img/Asymp3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "![AsympL1](./img/AsympL1.png)\n",
    "![AsympL2](./img/AsympL2.png)\n",
    "![AsympL3](./img/AsympL3.png)\n",
    "![AsympL4](./img/AsympL4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "![Exp1](./img/Exp1.png)\n",
    "![Exp2](./img/Exp2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "![Log1](./img/Log1.png)\n",
    "![Log2](./img/Log2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some quick notes on List-based datastructures - Refer to the excel sheet as supplement\n",
    "\n",
    "1. Array: Index based. Assume each memory cell stores data value. \n",
    "\n",
    "As per code basics Youtube video: the address of the very first memory cell of an array is assigned the index of 0. Further memory cells are assigned indices accordingly.\n",
    "\n",
    "As per Udacity video: we are storing the data value and the index for each element of the array.\n",
    "\n",
    "> Think of it as boxes lying next to each other. Each box contains data value and has an address called index.\n",
    ">\n",
    "> An array is a collection of items of same data type stored at contiguous memory locations. Due to same data type, the memory location of various elements can be obtained from the index by considering the offset and the memory requirement for each element. The offset is the memory location of the very first element and it can be assigned to the array variable.\n",
    ">\n",
    "> Insertion or deletion at the beginning: O(n). This is because all the elements needs to be shifted due to the contiguous memory allocation.\n",
    "> Insertion or deletion at the end: O(1) if the array limit is not reached. Otherwise O(n) if the array limit is reached, since new memory needs to be allocated to grow the dynamic array and the existing data needs to be copied to the new location which is a O(n) operation.\n",
    "> Inserttion or deletion in the middle: O(n) since elements after that needs to be shifted.\n",
    "\n",
    "> Lookup (get) and Set operation is O(1) as it is done through index. This is the case for Java's ArrayList at least.\n",
    ">\n",
    "> len() function should be O(n) too if we have to iterate over entire array to determine it's length, but programming languages usually store the value of length so it's usually O(1).\n",
    ">\n",
    "> In python-list, the append and pop-last operation is O(1), since the len() is O(1).\n",
    "\n",
    "2. Linked List: No index, each element is linked to next element. As per Udacity video: assume each memory cell stores data value and the memory address of the next element which is to be linked with current element.\n",
    "> Insert and delete at the beginning: O(1). \n",
    "> Insert and delete at the end: O(n) since the traversal is needed. O(1) if the tail pointer is maintained, this is the case in Java.\n",
    "> Insert and delete from the middle: O(n).\n",
    ">\n",
    "> Lookup at the beginning: O(1).\n",
    "> Lookup at the end: O(n) since the traversal is needed. O(1) if the tail pointer is maintained, this is the case in Java.\n",
    "> Lookup from the middle: O(n) as the traversal is needed.\n",
    ">\n",
    "> Linked list traversal i.e. linked list iteration is a O(n) operation.\n",
    ">\n",
    "> Doubly linked list has the pointers to the next element as well as the previous element. This allows to traverse the list in both directions now.\n",
    "\n",
    "3. Stacks: Last In, First Out. Push and Pop.\n",
    "> Can be implemented using LinkedList or Array. Insertion and Removal happens only on top of the stack. Since insertion and removal happens only on top, the operations become O(1).\n",
    "\n",
    "4. Queue: First In, First Out.\n",
    "> Enqueue operation adds data to the tail, while Dequeue operation removes data from the head and returns it to us. Peek operation let us see the data at Head, but doesn't remove it.\n",
    "\n",
    "5. Dequeue: Double Ended Queue\n",
    "> Allows Enqueue and Dequeue at head as well as tail. Sort of implements stack at head as well as tail, where Enqueue becomes Push while Dequeue becomes Pop when done at the same end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "\n",
    "**Naive Approach:** You compare every element to every other element until you have everything in order.\n",
    "\n",
    "**Inplace Sorting:** An inplace sorting algorithm just rearranges the elements in the data structure they are already in without needing to copy everything to a new data structure. Such algorithms have low space complexity\n",
    "\n",
    "**Bubble Sort / Sinking Sort:** It is kind of a naive approach of sorting. The idea is to go through the array, compare elements side by side and switch them when necessary. In each iteration, the largest element in the array will bubble on up to the top. Every iteration of n elements takes n-1 comparison steps. Number of iterations  is also n-1. So overall, it would take (n-1)<sup>2</sup> running time. This can be simplied to n<sup>2</sup>-2n+1. So, the worst-case and average-case efficiency would be O(n<sup>2</sup>).\n",
    "\n",
    "The best-case efficiency would be O(n) assuming that entire array is already sorted, so we just do n comparisons for one iteration only and we end-up with the sorted array.\n",
    "\n",
    "Since no new data structure or temporary storage is used, the space complexity is O(1) and this becomes Inplace Sorting algorithm.\n",
    "\n",
    "**Merge Sort:** The overall idea is for this algorithm is to you split a huge array down as much as possible then build it back up. We do the comparison and sorting while building the array back. This kind of approach is called ***Divide and Conquer*** since we break down the array, sort it and then build it up again.\n",
    "\n",
    "> P. S.: Binary Search is called ***Decrease and Conquer*** since we don't divide the array into smallest units but instead reduce it to half with every guess.\n",
    "\n",
    "Steps for Merge and Sort:\n",
    "\n",
    "[1] Break up the array into bunch of arrays with one element each.\n",
    "[2] Now make pairs of two-two arrays and sort the arrays within the pair.\n",
    "[3] Next combine four-four arrays and so on. Do an element-wise comparison. Refer to excel for more detail.\n",
    "\n",
    "*Time Efficiency:* O(nlogn)\n",
    "\n",
    "*Space Efficiency:* O(n)\n",
    "\n",
    "**Quick Sort:** In Quick Sort, pick one of the values in the array at random. Move all values larger than it, above it, and all values smaller than it, below it. The value that you pick initially is called a *pivot*. You continue recursively, picking a pivot in the upper and lower sections of the array, sorting them similarly until the whole array is sorted.\n",
    "\n",
    "- Most Efficient Sorting algorithm in most of the cases\n",
    "- Pick last element in the array as pivot.\n",
    "- Average and Best Case Efficiency: O(nlogn)\n",
    "- Worst Case Efficiency: O(n<sup>2</sup>)\n",
    "- Worst case happens when the array is already sorted or nearly sorted. Due to that, the pivot chosen ends up in same place and we any how compared all elements with pivot. So this algorithm should not be chosen for nearly sorted arrays.\n",
    "- For Best and Average case, the pivot will move close to the middle. Same will happen for pivots picked in lower half and lower half. Which will lead to logn part of the efficiency.\n",
    "- Optimization like running lower half and upper half sorting of array in parallel after splitting array from middle, can help improve the performance. Also, instead of selecting last element as the pivot, we can look at last few elements and then select median as the pivot.\n",
    "- In place sorting so Space complexity is O(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
