{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Search - All Credit to Khan Academy\n",
    "\n",
    "**Problem:** An array with n-elements is given. You need to find the index of element (target) that you need to search in the array. The array and the target can be provided as inputs to the Binary Search method, while the index of the target element should be returned as output. It should return -1, if no such element is found in the input array.\n",
    "\n",
    "**Psuedo code**\n",
    "\n",
    "\n",
    "\n",
    "> 1. Let min = 0 and max = n - 1\n",
    "2. If min > max, then the target element is not present in array. Return -1.\n",
    "3. Compute guess as the average of max and min, rounded down to an integer.\n",
    "4. If array[guess] equals target, then stop. You found it! Return guess.\n",
    "5. If the guess was too low, that is array[guess] < target, set min = guess + 1.\n",
    "6. Otherwise, the guess was too high. Set max = guess - 1.\n",
    "7. Go back to step 2.\n",
    "\n",
    "**Running Time:** The running time of binary search for an array with n elements can be given by log<sub>2</sub>n + 1. \n",
    "\n",
    "**E.g.:**\n",
    "\n",
    "For 32 elements, log<sub>2</sub>32 = 5, so it will take at most 6 iterations.\n",
    "\n",
    "For 1000 elements, log<sub>2</sub>512 = 9, so it will take at most 10 iterations.\n",
    "\n",
    "**P.S.** What if n isn't a power of 2? In that case, we can look at the closest lower power of 2. For an array whose length is 1000, the closest lower power of 2 is 512, which equals 2<sup>9</sup>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(input_array, target):\n",
    "    min_val = 0\n",
    "    max_val = len(input_array) - 1\n",
    "    while True:\n",
    "        if min_val > max_val:\n",
    "            return -1\n",
    "        else:\n",
    "            guess = int((min_val + max_val)/2)\n",
    "            if input_array[guess]==target:\n",
    "                return guess\n",
    "            else:\n",
    "                if input_array[guess]<target:\n",
    "                    min_val = guess + 1\n",
    "                else:\n",
    "                    max_val = guess - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_array = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n",
    "target = 67\n",
    "\n",
    "binary_search(input_array, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Efficiency of Binary Search\n",
    "\n",
    "It is observed that whenever the number of elements double, the number of iterations increase by 1 in case of Binary Search. Please refer to following table to understand this:\n",
    "\n",
    "**n**|-|0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16\n",
    "---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "**Iteration**|-|0|1|2|2|3|3|3|3|4|4|4|4|4|4|4|4|5\n",
    "\n",
    "From the above table, it can be observed that n+1 iterations are needed for 2<sup>n</sup> inputs.\n",
    "\n",
    "For e.g., 3 + 1 iterations are needed for 8 inputs. Let's ignore 1 for now. Then we get 3 iterations for 8 inputs.\n",
    "\n",
    "As we know:\n",
    "\n",
    "2<sup>3</sup> = 8\n",
    "\n",
    "log<sub>2</sub>(2<sup>3</sup>) = log<sub>2</sub>(8)\n",
    "\n",
    "3 = log<sub>2</sub>(8). This means 3 iterations can be written as log<sub>2</sub>(8). Also, 3 + 1 iterations are needed for 8 elements. So we can say that log<sub>2</sub>(8) + 1 iterations would be needed for 8 elements. And we can also replace 8 by n, as it would follow for any number in approximation.\n",
    "\n",
    "Thus the efficiency of binary search becomes log<sub>2</sub>(n) + 1. And then if we ignore 1, we can say it's efficiency would be log<sub>2</sub>(n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptotic Notations\n",
    "\n",
    "**Running time of an algorithm - Two Ideas**\n",
    "\n",
    "1. Think of running time as function of the size of its input. Since clearly, the running time increases with the size of input.\n",
    "2. We focus on how fast a function grows with the input size. This is called **rate of growth** of the algorithm's running time. We drop the less significant terms and coefficients from the running time equation, this gives us it's rate of growth.\n",
    "\n",
    "When we drop the constant coefficients and the less significant terms, we use asymptotic notation. We'll see three forms of it: big-Θ (Theta) notation, big-O notation, and big-Ω(Omega) notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big-Θ Notation - Tight Bound\n",
    "\n",
    "When we say that a particular running time is ***Θ(n)***, we are saying that once **n** gets large enough, the running time is at least *k<sub>1</sub> ⋅ n* and at most *k<sub>2</sub> ⋅ n* for some constants *k<sub>1</sub>* and *k<sub>2</sub>*. \n",
    "\n",
    "![Big-Theta on n](./img/Big-Theta1.png)\n",
    "\n",
    "For small values of n, we don't care how the running time compares with *k<sub>1</sub> ⋅ n* or *k<sub>2</sub> ⋅ n*, but once the n gets large enough or say it is on the right side of the dashed line, the running time must be sandwiched between *k<sub>1</sub> ⋅ n* and *k<sub>2</sub> ⋅ n*.\n",
    "\n",
    "As long as these constants *k<sub>1</sub>* and *k<sub>2</sub>* exists, we say that the running time is ***Θ(n)***. The running time has upper bound as well as the lower bound.\n",
    "\n",
    "We are not restricted to just *n* in Big-Θ notation, we could have any function such as n<sup>2</sup>, nlog<sub>2</sub>n, or any other function of n. Here is how to think of a running time that is *Θ(f(n))* of some function *f(n)*.\n",
    "\n",
    "![Big-Theta on f(n)](./img/Big-Theta2.png)\n",
    "\n",
    "Once *n* gets large enough, the running time is between *k<sub>1</sub> ⋅ f(n)* and *k<sub>2</sub> ⋅ f(n)*\n",
    "\n",
    "When using Big-Θ notation, we are saying that we have an asymptotically tight bound on the running time. Asymptotic because it matters only for large values of *n* or *f(n)* and tight bound because we nailed  the running time to within a constant factor above and below.\n",
    "\n",
    "**Formal Definition:**\n",
    "\n",
    "For f:N->R+, f = Θ(g(n)) if f = O(g) and f = Ω(g).\n",
    "****\n",
    "Some common Functions growing from lowest to highest:\n",
    "\n",
    "1. Θ(1) - Constant\n",
    "2. Θ(log<sub>2</sub>n) - Logarithmic\n",
    "3. Θ(n) - Linear\n",
    "4. Θ(nlog<sub>2</sub>n) - Linarithmic\n",
    "5. Θ(n<sup>2</sup>) - Polynomial\n",
    "6. Θ(n<sup>2</sup>log<sub>2</sub>n) - Polynomial Logarithmic\n",
    "7. Θ(n<sup>3</sup>) - Higher power of Polynomial\n",
    "8. Θ(2<sup>n</sup>) - Exponential\n",
    "9. Θ(n!) - Factorial\n",
    "\n",
    "****\n",
    "\n",
    "1. Constant functions\n",
    "2. Logarithmic functions (Higher the log base, lower the growth rate)\n",
    "3. Linear functions\n",
    "4. Linearithmic functions\n",
    "5. Polynomial functions\n",
    "6. Exponential functions\n",
    "\n",
    "![f(n) = Θ(g(n)) - 1](./img/FnGn1.png)\n",
    "![f(n) = Θ(g(n)) - 2](./img/FnGn2.png)\n",
    "![f(n) = Θ(g(n)) - 3](./img/FnGn3.png)\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big-O Notation - Upper Bound\n",
    "\n",
    "When we want to say that the running time could grow at the most to some value, but could grow more slowly, we use Big-O Notation. We use Big-O Notations for **asymptotic upper bounds**, since it bounds the growth of the running time from above, for large enough input sizes.\n",
    "\n",
    "If a running time is *O(f(n))*, then for large enough *n*, the running time is at most *k ⋅ f(n)* for some constant *k*.  Here's how to think of a running time that is *O(f(n))*:\n",
    "\n",
    "![Big-O on fn](./img/BigO.png)\n",
    "\n",
    "If we say that a running time is *Θ(f(n))* in a particular situation, then its also *O(f(n))*. The converse is not necessarily true.\n",
    "\n",
    "We can say that because the worst-case running time of binary search is Θ(log<sub>2</sub>n), it's also O(log<sub>2</sub>n).\n",
    "\n",
    "**Formal Definition:**\n",
    "\n",
    "Let f (n) and g(n) be functions N+ -> R+. f = O(g) if there is a constant c > 0 and n<sub>0</sub> belongs to N+ such that\n",
    "f (n) <= c * g(n) for all n >= n<sub>0</sub>. This means that f grows no faster than g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big-Ω Notation - Lower Bound\n",
    "\n",
    "Sometimes, we want to say that an algorithm takes at least a certain amount of time, without providing an upper bound. We use big-Ω notation in such situation.\n",
    "\n",
    "If a running time is *Ω(f(n))*, then for large enough *n*, the running time is at least *k ⋅ f(n)* for some constant k. Here's how to think of a running time that is Ω(f(n)):\n",
    "\n",
    "![Big-Ω](./img/BigOmega.png)\n",
    "\n",
    "We use big-Ω notation for asymptotic lower bounds, since it bounds the growth of the running time from below for large enough input sizes.\n",
    "\n",
    "Just as Θ(f(n)) automatically implies O(f(n)), it also automatically implies Ω(f(n)). We can say that the worst-case running time of binary search is Ω(log<sub>2</sub>n).\n",
    "\n",
    "**Formal Definition:**\n",
    "\n",
    "For f:N->R+, f = Ω(g) if there is a constant c > 0 and n<sub>0</sub> belongs to N such that f (n) >= c * g(n) for all n >= n<sub>0</sub>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following expressions, indicate whether f = O(g) or f = Ω(g) or f = Θ(g):\n",
    "\n",
    "No.| f(n) | g(n) | Notation\n",
    ":---:|:---:|:---:|:---:\n",
    "1.|f (n) = n - 10| g(n) = n - 20|f = Θ(g)\n",
    "2.|f (n) = n<sup>1/2</sup>| g (n) = n<sup>2/3</sup>|f = O(g)\n",
    "3.|f (n) = n2<sup>n</sup>| g(n) = 3<sup>n</sup>|f = Ω(g)\n",
    "4.|f (n) = 2<sup>n</sup>| g(n) = 2<sup>n+1</sup>|f = O(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Asymp1](./img/Asymp1.png)\n",
    "![Asymp2](./img/Asymp2.png)\n",
    "![Asymp3](./img/Asymp3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "![AsympL1](./img/AsympL1.png)\n",
    "![AsympL2](./img/AsympL2.png)\n",
    "![AsympL3](./img/AsympL3.png)\n",
    "![AsympL4](./img/AsympL4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "![Exp1](./img/Exp1.png)\n",
    "![Exp2](./img/Exp2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "![Log1](./img/Log1.png)\n",
    "![Log2](./img/Log2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some quick notes on List-based datastructures - Refer to the excel sheet as supplement\n",
    "\n",
    "1. Array: Index based. Assume each memory cell stores data value and array index for that value.\n",
    "> Think of it as boxes lying next to each other. Each box contains data value and information about it's index.\n",
    ">\n",
    "> Insertion and Removal is O(n) because n number of elements would need to be updated with there index values when a new element is added or removed.\n",
    ">\n",
    "> len() function should be O(n) too if we have to iterate over entire array to determine it's length, but programming languages usually store the value of length so it's usually O(1).\n",
    "\n",
    "2. Linked List: No index, each element is linked to next element. Assume each memory cell stores data value and the memory address of the next element which is to be linked with current element.\n",
    "> Insertion becomes O(1) because now we need to update just the memory address of newly added linked element in our current element. Also, the memory address of previously linked element must be placed into the newly added element, so that it now points to the previously linked element. Same is the case with removal.\n",
    "\n",
    "3. Stacks: Last In, First Out. Push and Pop.\n",
    "> Can be implemented using LinkedList or Array. Insertion and Removal happens only on top of the stack. Since insertion and removal happens only on top, the operations become O(1).\n",
    "\n",
    "4. Queue: First In, First Out.\n",
    "> Enqueue operation adds data to the tail, while Dequeue operation removes data from the head and returns it to us. Peek operation let us see the data at Head, but doesn't remove it.\n",
    "\n",
    "5. Dequeue: Double Ended Queue\n",
    "> Allows Enqueue and Dequeue at head as well as tail. Sort of implements stack at head as well as tail, where Enqueue becomes Push while Dequeue becomes Pop when done at the same end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
